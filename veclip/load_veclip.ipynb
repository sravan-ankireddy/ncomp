{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd2c8d8-35bf-425f-b94f-8bc16e895d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc9d4b4-aa14-4ab9-a8c2-2725bad00ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f595ab-9477-4bd4-86ca-41f438405761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the customized image preprocessor. Do not use the HF imagepreprocessor, otherwise the results will be different\n",
    "from clip_processor import image_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1efd6f-b4c1-4aae-95bc-d3a1bcfaf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "# download the checkpoints using `wget`, we use vecapdfn_clip_h14_336 as an example\n",
    "!wget https://docs-assets.developer.apple.com/ml-research/models/veclip/vecapdfn_clip_h14_336.zip -P checkpoints/\n",
    "!unzip checkpoints/vecapdfn_clip_h14_336.zip -d checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a5ce9-3915-42c1-abc3-504adbe20247",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"checkpoints/vecapdfn_clip_h14_336\"\n",
    "\n",
    "# load tokenizer and model\n",
    "# Note: The T5 tokenizer does not enforce a fixed maximum input length. Therefore, during usage, \n",
    "# if any warnings related to sequence length exceedance appear, they can generally be ignored.\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "print(f\"Loading model {MODEL_DIR} ...\")\n",
    "model = CLIPModel.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6467f8b-6263-4d92-a73a-274bd03382b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text model\n",
    "texts = [\"a photo of car\", \"a photo of two cats\"]\n",
    "text_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "text_outputs = model.text_model(**text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba8796-da82-4572-bcc9-04ff630896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab996df-573b-4a5c-ad5c-6cd6ee682eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# default image crop size\n",
    "crop_size = 224\n",
    "match = re.search(r'clip_h\\d+_(\\d+)', MODEL_DIR)\n",
    "if match:\n",
    "    crop_size = int(match.group(1))\n",
    "\n",
    "# vision model\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "pixel_values = image_preprocess([np.asarray(image)], crop_size=crop_size)\n",
    "\n",
    "vision_outputs = model.vision_model(pixel_values=pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635db4d8-7ee5-44a0-b61e-460a21e9db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbaf071-da79-4114-bf0e-2b5ee96ae57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-vision model\n",
    "outputs = model(**text_inputs, pixel_values=pixel_values)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523f7c8-5332-43bb-be1d-291011a95493",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)\n",
    "for prob, text in zip(probs[0], texts):\n",
    "    # Format and print the message\n",
    "    print(\"Probability for '{}' is {:.2%}\".format(text, prob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
